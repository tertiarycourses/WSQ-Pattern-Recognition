{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learner/Facilitator Guide - NICF - Pattern Recognition with Deep Learning - 3 Aug 2021.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xr7i2L08IIp5",
        "jEpIw2V0lmGF",
        "TO3WCmo2nB-Q",
        "3H1vHLHUmUId",
        "vNCS7L2amtYn",
        "av71E0Qcnwon",
        "D6mpaNGixeDu",
        "YRck3uIwyeLo",
        "6naCWJHxygNk",
        "u4eKE0nEyxfk",
        "kMaVg1zTKCIG",
        "1sH7Cj5WKqoI",
        "G_aEYm4PVptX",
        "AliO7-3ZVY0R",
        "6tZ-97-pcFIW",
        "dm-q7PEUcqbM",
        "RTcnxwuMdWST"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaZZDVcrO4gi"
      },
      "source": [
        "# Learner/Facilitator Guide for NICF - Pattern Recognition with Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr7i2L08IIp5"
      },
      "source": [
        "# Topic 1 Image Recognition with CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyEOuO2oByJX"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(\"Version: \", tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GoMUKLolzgg"
      },
      "source": [
        "## Image Classification Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD3qK9B4liVT"
      },
      "source": [
        "### Step 1: Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GVgi54-2gk2"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n",
        "\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEpIw2V0lmGF"
      },
      "source": [
        "#### ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtOhb7kR3AC4"
      },
      "source": [
        "batch_size = 100\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255) \n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255) \n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='categorical')\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO3WCmo2nB-Q"
      },
      "source": [
        "#### Visualize the raw images "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSpOtG-O3P-k"
      },
      "source": [
        "images,labels = next(train_data_gen)\n",
        "\n",
        "def plotImages(images, labels):\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
        "    axes = axes.flatten()\n",
        "    for img, label, ax in zip( images, labels, axes):\n",
        "        ax.imshow(img)\n",
        "        label = np.argmax(label)\n",
        "        label = 'cat' if label == 0 else 'dog' \n",
        "        print(label)\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plotImages(images[:5],labels[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H1vHLHUmUId"
      },
      "source": [
        "### Step 2: Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7r_aTsV3TMV"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape = (IMG_HEIGHT, IMG_WIDTH,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD3LNOqJjfZ-"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNCS7L2amtYn"
      },
      "source": [
        "### Step 3: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG2URME73ePz"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "epochs = 15\n",
        "history = model.fit(train_data_gen,epochs=epochs,validation_data=val_data_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av71E0Qcnwon"
      },
      "source": [
        "### Step 4: Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY2A6qRw3hee"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epoch = range(len(loss))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch,loss,label='loss')\n",
        "plt.plot(epoch,val_loss,label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch,acc,label='acc')\n",
        "plt.plot(epoch,val_acc,label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC5jis8VvsEv"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_path = '/content/test_dog.jpg'\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "img_tensor = image.img_to_array(img)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "img_tensor /= 255.\n",
        "\n",
        "preds = model.predict(img_tensor)\n",
        "output = np.argmax(preds)\n",
        "label = 'cat' if output == 0 else 'dog'\n",
        "\n",
        "plt.imshow(img_tensor[0])\n",
        "plt.show()\n",
        "print(preds)\n",
        "print('The model prediction is ',label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EDpYogDg_Ow"
      },
      "source": [
        "## Activity: Image Classification with CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dmgdj_ald5B"
      },
      "source": [
        "### Step 1: Import the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuKgJdsOgtmz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pUd6sBWhSHV"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "PATH = '/content/drive/MyDrive/dataset/hymenoptera_data'\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sLD9Dn9g0co"
      },
      "source": [
        "batch_size = 20\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255) \n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255) \n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='categorical')\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEJqoddsiGtI"
      },
      "source": [
        "sample_training_images, _ = next(train_data_gen)\n",
        "\n",
        "def plotImages(images_arr):\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
        "    axes = axes.flatten()\n",
        "    for img, ax in zip( images_arr, axes):\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plotImages(sample_training_images[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHQg8MW5lhqF"
      },
      "source": [
        "### Step 2: Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wRLNCNgiOPp"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape = (IMG_HEIGHT, IMG_WIDTH,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35EBNY4_lkfo"
      },
      "source": [
        "### Step 3: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6s4f0wmiWj5"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "epochs = 15\n",
        "history = model.fit(train_data_gen,epochs=epochs,validation_data=val_data_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqEpUJk_lnbw"
      },
      "source": [
        "### Step 4: Evalaute the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcAl9UkUkVLd"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epoch = range(len(loss))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch,loss,label='loss')\n",
        "plt.plot(epoch,val_loss,label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch,acc,label='acc')\n",
        "plt.plot(epoch,val_acc,label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VmYdrI0kZ82"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_path = '/content/348291597_ee836fbb1a.jpg'\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "img_tensor = image.img_to_array(img)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "img_tensor /= 255.\n",
        "\n",
        "preds = model.predict(img_tensor)\n",
        "output = np.argmax(preds)\n",
        "label = 'ant' if output == 0 else 'bee'\n",
        "\n",
        "plt.imshow(img_tensor[0])\n",
        "plt.show()\n",
        "print('The model prediction is ',label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgViJ2IO1ndb"
      },
      "source": [
        "# Topic 2 Methods to Solve Overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HZGzGWOGQhn"
      },
      "source": [
        "## Weight Regularizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7_b81kfggeV"
      },
      "source": [
        "#### Step 1: Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLqq53QBgj7k"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n",
        "\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkxUjTfTgxLg"
      },
      "source": [
        "batch_size = 100\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255) \n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255) \n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='categorical')\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkEPDE4CrSsf"
      },
      "source": [
        "#### Step 2: Define the Model with L2 Weight Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIe71UZ6Ex52"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape = (IMG_HEIGHT, IMG_WIDTH,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    # Conv2D(128, 3, padding='same', kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
        "    # MaxPooling2D(),\n",
        "    # Conv2D(128, 3, padding='same', kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
        "    # MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD8_C41FhH_b"
      },
      "source": [
        "#### Step 3: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZju7TMsFURb"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_data_gen,epochs=epochs,validation_data=val_data_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nupi8w3-IZ6G"
      },
      "source": [
        "#### Step 4: Evalaute the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNeqV73JGoTI"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epoch = range(len(loss))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch,loss,label='loss')\n",
        "plt.plot(epoch,val_loss,label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch,acc,label='acc')\n",
        "plt.plot(epoch,val_acc,label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8p6qllO2YLR"
      },
      "source": [
        "#### Step 2: Define the Model with L1 Weight Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2-O3eX-H5SP"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l1(0.001), input_shape = (IMG_HEIGHT, IMG_WIDTH,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', kernel_regularizer=regularizers.l1(0.001), activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    # Conv2D(128, 3, padding='same', kernel_regularizer=regularizers.l1(0.001), activation='relu'),\n",
        "    # MaxPooling2D(),\n",
        "    # Conv2D(128, 3, padding='same', kernel_regularizer=regularizers.l1(0.001), activation='relu'),\n",
        "    # MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro95F9LEIhb9"
      },
      "source": [
        "#### Step 3: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86Hm9YwqH_LJ"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_data_gen,epochs=epochs,validation_data=val_data_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjjokHfAIj4W"
      },
      "source": [
        "#### Step 4: Evalaute the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STgaNNW7IL24"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epoch = range(len(loss))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch,loss,label='loss')\n",
        "plt.plot(epoch,val_loss,label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch,acc,label='acc')\n",
        "plt.plot(epoch,val_acc,label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV7O_gPPIJFx"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWaQiuA-Lffs"
      },
      "source": [
        "#### Step 2: Define the Model with Dropout Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkPvPWENLwqa"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape = (IMG_HEIGHT, IMG_WIDTH,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    # Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    # MaxPooling2D(),\n",
        "    # Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    # MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JR0TiHXIpBY"
      },
      "source": [
        "#### Step 3: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJsEedg5MBr8"
      },
      "source": [
        "epochs = 15\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_data_gen,epochs=epochs,validation_data=val_data_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUBw2mfOIrYq"
      },
      "source": [
        "#### Step 4: Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrp6D1LuMD7Q"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epoch = range(len(loss))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch,loss,label='loss')\n",
        "plt.plot(epoch,val_loss,label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch,acc,label='acc')\n",
        "plt.plot(epoch,val_acc,label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# import pandas as pd\n",
        "\n",
        "# history_df = pd.DataFrame(history.history)\n",
        "# history_df.loc[:, ['loss', 'val_loss']].plot();\n",
        "# history_df.loc[:, ['accuracy', 'val_accuracy']].plot();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWgPlYfVILzT"
      },
      "source": [
        "## Data Augumenation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6mpaNGixeDu"
      },
      "source": [
        "#### Step 1: Load the Data with Data Augumentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkyUlE-g4KN6"
      },
      "source": [
        "image_gen_train = ImageDataGenerator(\n",
        "                    rescale=1./255,\n",
        "                    rotation_range=45,\n",
        "                    width_shift_range=.15,\n",
        "                    height_shift_range=.15,\n",
        "                    horizontal_flip=True,\n",
        "                    zoom_range=0.5\n",
        "                    )\n",
        "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data_gen = image_gen_train.flow_from_directory(batch_size=batch_size,\n",
        "                                                     directory=train_dir,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                     class_mode='categorical')\n",
        "\n",
        "val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,\n",
        "                                                 directory=validation_dir,\n",
        "                                                 target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                 class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMtG5LNYOMO-"
      },
      "source": [
        "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
        "def plotImages(images_arr):\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
        "    axes = axes.flatten()\n",
        "    for img, ax in zip( images_arr, axes):\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
        "plotImages(augmented_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRck3uIwyeLo"
      },
      "source": [
        "#### Step 2: Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QU9CT1u4WYH"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape = (IMG_HEIGHT, IMG_WIDTH,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    # Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    # MaxPooling2D(),\n",
        "    # Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    # MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6naCWJHxygNk"
      },
      "source": [
        "#### Step 3: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWN9-NfR4Z5y"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_data_gen,epochs=epochs,validation_data=val_data_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4eKE0nEyxfk"
      },
      "source": [
        "#### Step 4: Evalaute the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTVpfC2X4erj"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epoch = range(len(loss))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch,loss,label='loss')\n",
        "plt.plot(epoch,val_loss,label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch,acc,label='acc')\n",
        "plt.plot(epoch,val_acc,label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMaVg1zTKCIG"
      },
      "source": [
        "## Activity: Solve Overfitting Issue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2raz9G07eX7"
      },
      "source": [
        "### Step 1: Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIw08yAA7f5J"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrV5JzSk78nB"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "PATH = '/content/drive/MyDrive/dataset/hymenoptera_data'\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPhfDE1l8ENh"
      },
      "source": [
        "batch_size = 20\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "image_gen_train = ImageDataGenerator(\n",
        "                    rescale=1./255,\n",
        "                    rotation_range=45,\n",
        "                    width_shift_range=.15,\n",
        "                    height_shift_range=.15,\n",
        "                    horizontal_flip=True,\n",
        "                    zoom_range=0.5\n",
        "                    )\n",
        "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data_gen = image_gen_train.flow_from_directory(batch_size=batch_size,\n",
        "                                                     directory=train_dir,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                     class_mode='categorical')\n",
        "\n",
        "val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,\n",
        "                                                 directory=validation_dir,\n",
        "                                                 target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                 class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06kkxusg7dAP"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotImages(images_arr):\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
        "    axes = axes.flatten()\n",
        "    for img, ax in zip( images_arr, axes):\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
        "plotImages(augmented_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqDXc56I7jpY"
      },
      "source": [
        "### Step 2: Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB2-2ZnP8T9Z"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape = (IMG_HEIGHT, IMG_WIDTH,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dropout(0.5),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrgZi16h7t2e"
      },
      "source": [
        "### Step 3: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn9Lctpx8rQ8"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_augmentation = model.fit(train_data_gen,epochs=epochs,validation_data=val_data_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaCjOF6Q7wwQ"
      },
      "source": [
        "### Step 4: Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nOtnEl5-0Y0"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epoch = range(len(loss))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch,loss,label='loss')\n",
        "plt.plot(epoch,val_loss,label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch,acc,label='acc')\n",
        "plt.plot(epoch,val_acc,label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8M-eZxlU1Ql"
      },
      "source": [
        "# Topic 3 Functional Keras API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjzGIe8RcUDU"
      },
      "source": [
        "## Layer as Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkQ7H5seBXD_"
      },
      "source": [
        "#### Step 1: Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_H2NX6Lb0mC"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n",
        "\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1My_b-RucG3x"
      },
      "source": [
        "batch_size = 20\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255) \n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255) \n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='categorical')\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG_dDpJMBlll"
      },
      "source": [
        "#### Step 2: Define the Model with Functional API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhDM8qHIcQWA"
      },
      "source": [
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# model = Sequential([\n",
        "#     Conv2D(32, 3, padding='same', activation='relu', input_shape = (IMG_HEIGHT, IMG_WIDTH,3)),\n",
        "#     MaxPooling2D(),\n",
        "#     Conv2D(64, 3, padding='same', activation='relu'),\n",
        "#     MaxPooling2D(),\n",
        "#     Conv2D(128, 3, padding='same', activation='relu'),\n",
        "#     MaxPooling2D(),\n",
        "#     Conv2D(128, 3, padding='same', activation='relu'),\n",
        "#     MaxPooling2D(),\n",
        "#     Flatten(),\n",
        "#     Dense(512, activation='relu'),\n",
        "#     Dense(2, activation='softmax')\n",
        "# ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njf-bL8HcfWM"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "inputs = tf.keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH,3))\n",
        "x = Conv2D(32, 3, padding='same', activation='relu')(inputs)\n",
        "x = MaxPooling2D()(x)\n",
        "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "x = MaxPooling2D()(x)\n",
        "x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
        "x = MaxPooling2D()(x)\n",
        "x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
        "x = MaxPooling2D()(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "outputs = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6t4zUIBBsUH"
      },
      "source": [
        "#### Step 3: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjiVw0bmeDyY"
      },
      "source": [
        "epochs=15\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_data_gen,epochs=epochs,validation_data=val_data_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk8TPI8ZBwE7"
      },
      "source": [
        "#### Step 4: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrwT5BTYeuRf"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epoch = range(len(loss))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch,loss,label='loss')\n",
        "plt.plot(epoch,val_loss,label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch,acc,label='acc')\n",
        "plt.plot(epoch,val_acc,label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sH7Cj5WKqoI"
      },
      "source": [
        "### Activity: Functional API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_2EuMEOERaW"
      },
      "source": [
        "#### Step 1: Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIikWWgIgMKg"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7yyGamQgZot"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "PATH = '/content/drive/MyDrive/dataset/hymenoptera_data'\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SpQxJ73ggKv"
      },
      "source": [
        "batch_size = 20\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255) \n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255) \n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='categorical')\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_x6RmGWEWUz"
      },
      "source": [
        "#### Step 2: Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-VDJbNbgwvZ"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "inputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH,3))\n",
        "x = Conv2D(32, 3, padding='same', activation='relu')(inputs)\n",
        "x = MaxPooling2D()(x)\n",
        "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "x = MaxPooling2D()(x)\n",
        "x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
        "x = MaxPooling2D()(x)\n",
        "x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
        "x = MaxPooling2D()(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "outputs = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSQ5sXbsEY91"
      },
      "source": [
        "#### Step 3: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT4u4jVNhD-a"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_data_gen,epochs=epochs,validation_data=val_data_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9THMAsXREbtj"
      },
      "source": [
        "#### Step 4: Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wGHJ3FMhPWR"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epoch = range(len(loss))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch,loss,label='loss')\n",
        "plt.plot(epoch,val_loss,label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch,acc,label='acc')\n",
        "plt.plot(epoch,val_acc,label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We_i4QsMJhCG"
      },
      "source": [
        "## Feature Map Visualizaton\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igKP2xKUKOor"
      },
      "source": [
        "#### Step 1: Load the Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC8Ncg4XuRN9"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_path = '/content/test.jpeg'\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "img_tensor = image.img_to_array(img)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "img_tensor /= 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPgo9Uj2mn25"
      },
      "source": [
        "for i, layer in enumerate(model.layers):\n",
        "    print(i,layer.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maeXjywYKw11"
      },
      "source": [
        "#### Step 2: Visualize Feature Map for Layer 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSET3--n-JSn"
      },
      "source": [
        "activation_model = Model(inputs=model.inputs, outputs=model.layers[1].output)\n",
        "activation = activation_model.predict(img_tensor)\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "for i in range(16):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.imshow(activation[0, :, :, i])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg2PwjuRKhMb"
      },
      "source": [
        "#### Step 2: Visualize Feature Map for Layer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YF-rx5uwFNz"
      },
      "source": [
        "activation_model = Model(inputs=model.inputs, outputs=model.layers[3].output)\n",
        "activation = activation_model.predict(img_tensor)\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "for i in range(64):\n",
        "    plt.subplot(8,8,i+1)\n",
        "    plt.imshow(activation[0, :, :, i])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJHTJpviLzPX"
      },
      "source": [
        "#### Step 2: Visualize Feature Map for Layer 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnyQ0qURnVym"
      },
      "source": [
        "activation_model = Model(inputs=model.inputs, outputs=model.layers[5].output)\n",
        "activation = activation_model.predict(img_tensor)\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "for i in range(64):\n",
        "    plt.subplot(8,8,i+1)\n",
        "    plt.imshow(activation[0, :, :, i])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3f_mhKwL0-Q"
      },
      "source": [
        "#### Step 2: Visualize Feature Map for Layer 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys7TIUoaLCqW"
      },
      "source": [
        "activation_model = Model(inputs=model.inputs, outputs=model.layers[7].output)\n",
        "activation = activation_model.predict(img_tensor)\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "for i in range(64):\n",
        "    plt.subplot(8,8,i+1)\n",
        "    plt.imshow(activation[0, :, :, i])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtPpxGzPstu0"
      },
      "source": [
        "### Activity: Feature Map Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2uY2WJtKSRU"
      },
      "source": [
        "#### Step 1: Load the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLN00vK_J_FK"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ng8mYygKCAx"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "PATH = '/content/drive/MyDrive/dataset/hymenoptera_data'\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjAczVTGKFn0"
      },
      "source": [
        "batch_size = 20\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255) \n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255) \n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='categorical')\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeP6VOk_LM3p"
      },
      "source": [
        "#### Step 2: Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bBuTP-ZKImD"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "inputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH,3))\n",
        "x = Conv2D(32, 3, padding='same', activation='relu')(inputs)\n",
        "x = MaxPooling2D()(x)\n",
        "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "x = MaxPooling2D()(x)\n",
        "x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
        "x = MaxPooling2D()(x)\n",
        "x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
        "x = MaxPooling2D()(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "outputs = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMNu-A-zLPxu"
      },
      "source": [
        "#### Step 3: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Btk12c5xKLyw"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_data_gen,epochs=epochs,validation_data=val_data_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SSSrTMFLS1j"
      },
      "source": [
        "#### Step 4: Visualize the Feature Maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkj2LW13wPCy"
      },
      "source": [
        "for i, layer in enumerate(model.layers):\n",
        "    print(i,layer.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI3AjEG8wi3x"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_path = '/content/348291597_ee836fbb1a.jpg'\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "img_tensor = image.img_to_array(img)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "img_tensor /= 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHuLYMC02C63"
      },
      "source": [
        "layer_outputs = [layer.output for layer in model.layers[0:1]]\n",
        "activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
        "activation = activation_model.predict(img_tensor)\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "for i in range(16):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.imshow(activation[0, :, :, i])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgpKOgzyc3S3"
      },
      "source": [
        "layer_outputs = [layer.output for layer in model.layers[6:7]]\n",
        "activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
        "activation = activation_model.predict(img_tensor)\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "for i in range(128):\n",
        "    plt.subplot(32,8,i+1)\n",
        "    plt.imshow(activation[0, :, :, i])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nbefACoDOxb"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape = (IMG_HEIGHT, IMG_WIDTH,3)),\n",
        "    #MaxPooling2D(),\n",
        "    Conv2D(16, 3, padding='same', activation='relu'),\n",
        "    #MaxPooling2D(),\n",
        "    Conv2D(16, 3, padding='same', activation='relu'),\n",
        "    #MaxPooling2D(),\n",
        "    Conv2D(16, 3, padding='same', activation='relu'),\n",
        "    #MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxjSYaS-DiHC"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "epochs = 15\n",
        "history = model.fit(train_data_gen,epochs=epochs,validation_data=val_data_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HeWq_7zY6oh"
      },
      "source": [
        "# Topic 4 Transfer Learning for Small Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVy4LTOaVTTV"
      },
      "source": [
        "## Test the Pre-trained Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkdwKtNbx9WT"
      },
      "source": [
        "### Renset Pre-trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vczf7wydxzf1"
      },
      "source": [
        "# Resnet Pre-trained Model\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "\n",
        "model = ResNet50(weights='imagenet')\n",
        "\n",
        "img_path = '/content/348291597_ee836fbb1a.jpg'\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "preds = model.predict(x)\n",
        "print('Predicted:', decode_predictions(preds, top=3)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCwzkI5JyDYP"
      },
      "source": [
        "### Activity: Pre-trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1Jwg2kZx433"
      },
      "source": [
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "\n",
        "model = MobileNetV2(weights='imagenet')\n",
        "\n",
        "img_path = '/content/guess3.jpg'\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "preds = model.predict(x)\n",
        "print('Predicted:', decode_predictions(preds, top=3)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_aEYm4PVptX"
      },
      "source": [
        "## Feature Extraction & Fine Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRvTRHqgVz9a"
      },
      "source": [
        "### Step 1: Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEuOIRD0VnTc"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n",
        "\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_jEgw_HVwGf"
      },
      "source": [
        "batch_size = 20\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255) \n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255) \n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='categorical')\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPipA2mPWACH"
      },
      "source": [
        "### Step 2: Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC6b5TlezSh4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow. keras.preprocessing import image\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5oFy8mPzroV"
      },
      "source": [
        "base_model=MobileNet(weights='imagenet',include_top=False, input_shape=(150, 150, 3)) \n",
        "\n",
        "x=base_model.output\n",
        "x=GlobalAveragePooling2D()(x) \n",
        "x=Dense(512,activation='relu')(x) \n",
        "preds=Dense(2,activation='softmax')(x) \n",
        "\n",
        "model=Model(inputs=base_model.input,outputs=preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_SuYmC4k6dI"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5Uf1n7ul76f"
      },
      "source": [
        "#### Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eMSbERplvws"
      },
      "source": [
        "base_model.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvxkgU8Jl_5T"
      },
      "source": [
        "#### Fine Tunning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0G65Lqr0M1_"
      },
      "source": [
        "# for layer in model.layers[:20]:\n",
        "#     layer.trainable=False\n",
        "# for layer in model.layers[20:]:\n",
        "#     layer.trainable=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coVbrIe9WXzW"
      },
      "source": [
        "### Step 3: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGOyEsScWd_y"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_data_gen,epochs=epochs,validation_data=val_data_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ5Bf_mtWjfR"
      },
      "source": [
        "### Step 4: Evalaute the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JAVJ6-Ym6wk"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epoch = range(len(loss))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch,loss,label='loss')\n",
        "plt.plot(epoch,val_loss,label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch,acc,label='acc')\n",
        "plt.plot(epoch,val_acc,label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AliO7-3ZVY0R"
      },
      "source": [
        "### Step 5: Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f_HxRBW0aGd"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_path = 'test_cat.jpg'\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "img_tensor = image.img_to_array(img)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "img_tensor /= 255.\n",
        "\n",
        "preds = model.predict(img_tensor)\n",
        "output = np.argmax(preds)\n",
        "label = 'cat' if output == 0 else 'dog'\n",
        "\n",
        "plt.imshow(img_tensor[0])\n",
        "plt.show()\n",
        "print('The model prediction is ',label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz3J0aNwurpH"
      },
      "source": [
        "## Activity: Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFbS8PerYhV2"
      },
      "source": [
        "### Step 1: Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbzK7o6bvBKu"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMnmd1nQvFR1"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "PATH = '/content/drive/MyDrive/dataset/hymenoptera_data'\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gYSJX5NvMc2"
      },
      "source": [
        "batch_size = 20\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255) \n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255) \n",
        "\n",
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='categorical')\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ8v1QCMYmJD"
      },
      "source": [
        "### Step 2: Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq6_PpsvvhcF"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow. keras.preprocessing import image\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4aDUH0kvoZI"
      },
      "source": [
        "base_model=MobileNet(weights='imagenet',include_top=False, input_shape=(150, 150, 3)) \n",
        "\n",
        "x=base_model.output\n",
        "x=GlobalAveragePooling2D()(x) \n",
        "x=Dense(512,activation='relu')(x) \n",
        "preds=Dense(2,activation='softmax')(x) \n",
        "\n",
        "model=Model(inputs=base_model.input,outputs=preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQQ00Y_cvsjr"
      },
      "source": [
        "base_model.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3SQA-DWY3_h"
      },
      "source": [
        "### Step 3: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp9rtyVNv3hh"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_data_gen,epochs=epochs,validation_data=val_data_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFB6L-JGZGNA"
      },
      "source": [
        "### Step 4: Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_jwxSBk2XaO"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epoch = range(len(loss))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch,loss,label='loss')\n",
        "plt.plot(epoch,val_loss,label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch,acc,label='acc')\n",
        "plt.plot(epoch,val_acc,label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuSEeNkkZTe9"
      },
      "source": [
        "### Step 5: Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw3kKZOuZQp5"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_path = '/content/348291597_ee836fbb1a.jpg'\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "img_tensor = image.img_to_array(img)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "img_tensor /= 255.\n",
        "\n",
        "preds = model.predict(img_tensor)\n",
        "output = np.argmax(preds)\n",
        "label = 'cat' if output == 0 else 'dog'\n",
        "\n",
        "plt.imshow(img_tensor[0])\n",
        "plt.show()\n",
        "print('The model prediction is ',label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLlwee1LY0CA"
      },
      "source": [
        "# Topic 5 Text Classification with RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46Bs8B1gcbpm"
      },
      "source": [
        "## RNN/LSTM/GRU and Input Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgE8sw3gcUM0"
      },
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "batch = 5 \n",
        "feature = 10 # could be the embedding size\n",
        "hidden_size = 20 \n",
        "timesteps = 3 # sequence length \n",
        "\n",
        "inputs = np.random.randn(batch, timesteps, feature)\n",
        "rnn = tf.keras.layers.SimpleRNN(hidden_size)\n",
        "\n",
        "h_out = rnn(inputs)\n",
        "print('Output size (batch, hidden_size) = ', h_out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEjBqqgccWbF"
      },
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "batch = 5 \n",
        "feature = 10 # could be the embedding size\n",
        "hidden_size = 20 \n",
        "timesteps = 3 # sequence length \n",
        "\n",
        "inputs = np.random.randn(batch, timesteps, feature)\n",
        "lstm = tf.keras.layers.LSTM(hidden_size)\n",
        "\n",
        "h_out = lstm(inputs)\n",
        "print('Output size (batch, hidden_size) = ', h_out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbEPZQjacYjq"
      },
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "batch = 5 \n",
        "feature = 10 # could be the embedding size\n",
        "hidden_size = 20 \n",
        "timesteps = 3 # sequence length \n",
        "\n",
        "inputs = np.random.randn(batch, timesteps, feature)\n",
        "gru = tf.keras.layers.GRU(hidden_size)\n",
        "\n",
        "h_out = gru(inputs)\n",
        "print('Output size (batch, hidden_size) = ', h_out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4RGqLxeclQM"
      },
      "source": [
        "## Time Series Forcasting with RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aypqhdw2cozk"
      },
      "source": [
        "### Time Series Forecasting (Airplane Passengers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B9tylB6cyj4"
      },
      "source": [
        "#### Step 1: Preprocess Data (Air Passengers Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58RGWJpYckZ5"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlH-VpbCc1Ar"
      },
      "source": [
        "dataset = pd.read_csv('airline-passengers.csv')\n",
        "dataset.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMI_4fuGc3FK"
      },
      "source": [
        "dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBfSNcMMc5LQ"
      },
      "source": [
        "dataset = dataset.iloc[:,1:2].values\n",
        "\n",
        "plt.plot(dataset)\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Passengers')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5IA63Suc8az"
      },
      "source": [
        "def sliding_window(data, seq_length):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data)-seq_length-1):\n",
        "        _x = data[i:(i+seq_length)]\n",
        "        _y = data[i+seq_length]\n",
        "        x.append(_x)\n",
        "        y.append(_y)\n",
        "\n",
        "    return np.array(x),np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGzYhiK7c-m5"
      },
      "source": [
        "sc = MinMaxScaler()\n",
        "dataset = sc.fit_transform(dataset)\n",
        "\n",
        "timesteps = 4\n",
        "X,y = sliding_window(dataset, timesteps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UPO_FmSdArI"
      },
      "source": [
        "train_size = int(len(y) * 0.67)\n",
        "test_size = int(len(y)) - train_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMGX_A-rdC1H"
      },
      "source": [
        "X_train = X[0:train_size]\n",
        "y_train = y[0:train_size]\n",
        "\n",
        "X_test = X[train_size:len(X)]\n",
        "y_test = y[train_size:len(y)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CSHXiVc8bsK"
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFHs1G5pdE3G"
      },
      "source": [
        "input_size = 1\n",
        "hidden_size = 5\n",
        "\n",
        "# inputs: A 3D tensor with shape [batch, timesteps, feature].\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], timesteps, input_size))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], timesteps, 1, input_size))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mhI7OaEdKj_"
      },
      "source": [
        "#### Step 2: Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNSGsV9edMvy"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(hidden_size, activation='tanh',input_shape=(timesteps,input_size)))\n",
        "model.add(Dense(1,activation='linear'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UmV0JDBdOtX"
      },
      "source": [
        "#### Step 3: Define Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd79Dt-KdQ9f"
      },
      "source": [
        "model.compile(loss='mse', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzQXBQhfdU9w"
      },
      "source": [
        "#### Step 4: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K69uq5dDdVif"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvfifV59dY4t"
      },
      "source": [
        "#### Step 5: Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxPSTx_TdcSj"
      },
      "source": [
        "import pandas as pd \n",
        "\n",
        "loss_df = pd.DataFrame(history.history)\n",
        "loss_df['loss'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMN2YNFGdewZ"
      },
      "source": [
        "yhat = model(X)\n",
        "\n",
        "yhat = sc.inverse_transform(yhat)\n",
        "y_ = sc.inverse_transform(y)\n",
        "\n",
        "plt.axvline(x=train_size, c='g', linestyle='--')\n",
        "\n",
        "plt.plot(y_,'b',label='actual')\n",
        "plt.plot(yhat,'r',label='prediction')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Airline Passengers')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eews32uz-U1s"
      },
      "source": [
        "### Activity: Sales Price Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5srntLjB3Fj"
      },
      "source": [
        "#### Step 1: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZGSfd0B-X7E"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "dataset = pd.read_csv('shampoo.csv')\n",
        "dataset.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEsaY3hy-h_q"
      },
      "source": [
        "dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMsQZDgyAVTG"
      },
      "source": [
        "dataset = dataset.iloc[:,1:2].values\n",
        "\n",
        "plt.plot(dataset)\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0gaF4wHApAy"
      },
      "source": [
        "def sliding_window(data, seq_length):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data)-seq_length-1):\n",
        "        _x = data[i:(i+seq_length)]\n",
        "        _y = data[i+seq_length]\n",
        "        x.append(_x)\n",
        "        y.append(_y)\n",
        "\n",
        "    return np.array(x),np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjC9Jup5AwnX"
      },
      "source": [
        "sc = MinMaxScaler()\n",
        "dataset = sc.fit_transform(dataset)\n",
        "\n",
        "timesteps = 4\n",
        "X,y = sliding_window(dataset, timesteps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSJNCqHfA1yR"
      },
      "source": [
        "train_size = int(len(y) * 0.67)\n",
        "test_size = int(len(y)) - train_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxB_Usq1A7x5"
      },
      "source": [
        "X_train = X[0:train_size]\n",
        "y_train = y[0:train_size]\n",
        "\n",
        "X_test = X[train_size:len(X)]\n",
        "y_test = y[train_size:len(y)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQbRipLYA_5i"
      },
      "source": [
        "train_size = int(len(y) * 0.67)\n",
        "test_size = int(len(y)) - train_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzIhF6jdBCo9"
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ahRDn4tBDhj"
      },
      "source": [
        "input_size = 1\n",
        "hidden_size = 5\n",
        "\n",
        "# inputs: A 3D tensor with shape [batch, timesteps, feature].\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], timesteps, input_size))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], timesteps, 1, input_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aX27HXcB7mB"
      },
      "source": [
        "#### Step 2: Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlySt09oBNvI"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(hidden_size, activation='tanh',input_shape=(timesteps,input_size)))\n",
        "model.add(Dense(1,activation='linear'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2rHC-sDB-OA"
      },
      "source": [
        "#### Step 3: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-cQ5DpJBQ5_"
      },
      "source": [
        "model.compile(loss='mse', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYsxuDAnBXNr"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BX0JD9KCDXy"
      },
      "source": [
        "#### Step 4: Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuL5kCpyBjsa"
      },
      "source": [
        "import pandas as pd \n",
        "\n",
        "loss_df = pd.DataFrame(history.history)\n",
        "loss_df['loss'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEiaYPPpBn0I"
      },
      "source": [
        "yhat = model(X)\n",
        "\n",
        "yhat = sc.inverse_transform(yhat)\n",
        "y_ = sc.inverse_transform(y)\n",
        "\n",
        "plt.axvline(x=train_size, c='g', linestyle='--')\n",
        "\n",
        "plt.plot(y_,'b',label='actual')\n",
        "plt.plot(yhat,'r',label='prediction')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Airline Passengers')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hDc_rCmdj-Y"
      },
      "source": [
        "### Activity: Stock Price Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hosmF9HZdn_C"
      },
      "source": [
        "#### Step 1: Preprocess the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3gGKW5Cdp17"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw183kjMdrr3"
      },
      "source": [
        "dataset = pd.read_csv('DBS.csv',usecols=['Date','Close'])\n",
        "\n",
        "dataset.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5guCkWcgdtSo"
      },
      "source": [
        "dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2eME0oadvCF"
      },
      "source": [
        "dataset = dataset.iloc[:,1:2].values\n",
        "\n",
        "plt.plot(dataset)\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuMHbwKsdw1z"
      },
      "source": [
        "def sliding_window(data, seq_length):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data)-seq_length-1):\n",
        "        _x = data[i:(i+seq_length)]\n",
        "        _y = data[i+seq_length]\n",
        "        x.append(_x)\n",
        "        y.append(_y)\n",
        "\n",
        "    return np.array(x),np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWNfgDRGdyoj"
      },
      "source": [
        "sc = MinMaxScaler()\n",
        "dataset = sc.fit_transform(dataset)\n",
        "\n",
        "timesteps = 4\n",
        "X,y = sliding_window(dataset, timesteps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPdmTfAxd0Wz"
      },
      "source": [
        "train_size = int(len(y) * 0.67)\n",
        "test_size = int(len(y)) - train_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIcfd7uVd2Ut"
      },
      "source": [
        "X_train = X[0:train_size]\n",
        "y_train = y[0:train_size]\n",
        "\n",
        "X_test = X[train_size:len(X)]\n",
        "y_test = y[train_size:len(y)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XwEt4RFd4Sw"
      },
      "source": [
        "input_size = 1\n",
        "hidden_size = 5\n",
        "\n",
        "# inputs: A 3D tensor with shape [batch, timesteps, feature].\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], timesteps, input_size))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], timesteps, 1, input_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNgK4aKmd6db"
      },
      "source": [
        "#### Step 2: Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJkhtwUVd7O4"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(hidden_size, activation='tanh',input_shape=(timesteps,input_size)))\n",
        "model.add(Dense(1,activation='linear'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-2p8bgCd9HI"
      },
      "source": [
        "#### Step 3: Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HABPQcUpd_EV"
      },
      "source": [
        "model.compile(loss='mse', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7ElAsR0eBAF"
      },
      "source": [
        "#### Step 4: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HHB6hwAeDDD"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKMWiarYeGd3"
      },
      "source": [
        "#### Step 5: Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKYn2qwDeInB"
      },
      "source": [
        "import pandas as pd \n",
        "\n",
        "loss_df = pd.DataFrame(history.history)\n",
        "loss_df['loss'].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9l6fvEIeLDf"
      },
      "source": [
        "yhat = model(X)\n",
        "\n",
        "yhat = sc.inverse_transform(yhat)\n",
        "y_ = sc.inverse_transform(y)\n",
        "\n",
        "plt.axvline(x=train_size, c='g', linestyle='--')\n",
        "\n",
        "plt.plot(y_,'b',label='actual')\n",
        "plt.plot(yhat,'r',label='prediction')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Stock Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oftG5ka_YMyS"
      },
      "source": [
        "## Text Classifcaiton With RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHh4HhSzZaxl"
      },
      "source": [
        "### IMDB Sentimental Analysis Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEmqPrAWa75W"
      },
      "source": [
        "#### Step 1: Load the IMDB dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6eJ1XvJYREL"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_words = 10000  \n",
        "seq_length = 80  \n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=seq_length)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=seq_length)\n",
        "print('input_train shape:', x_train.shape)\n",
        "print('input_test shape:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tZ-97-pcFIW"
      },
      "source": [
        "#### Step 2: Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s62nbYQoY5Bz"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow. keras.layers import LSTM\n",
        "\n",
        "hidden_size = 32\n",
        "embedding_size = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_size))\n",
        "model.add(LSTM(hidden_size,activation='tanh'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm-q7PEUcqbM"
      },
      "source": [
        "#### Step 3: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DZO_FV7edM6"
      },
      "source": [
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8MPZorXZFhp"
      },
      "source": [
        "batch = 250\n",
        "num_epoch = 10 \n",
        "history = model.fit(x_train, y_train, epochs=num_epoch, batch_size=batch,validation_data=(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTcnxwuMdWST"
      },
      "source": [
        "#### Step 4: Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwp6F2TQZNN7"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epoch = range(len(loss))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch,loss,label='loss')\n",
        "plt.plot(epoch,val_loss,label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch,acc,label='acc')\n",
        "plt.plot(epoch,val_acc,label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6WcUpdXetqp"
      },
      "source": [
        "import numpy as np\n",
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "\n",
        "#text = \"This movie is lousy and horrible I feel very sad\"\n",
        "#text =\" This movie is very good and nice I feel very happy\"\n",
        "# text = \"This movie is lousy and horrible It is wasting my money\"\n",
        "text =\"This movie is good worth the money I love it\"\n",
        "tokens = text.lower().split()\n",
        "token_index = [word_index[word] for word in tokens]\n",
        "token_index = np.array(token_index)\n",
        "token_index = token_index[np.newaxis,:]\n",
        "\n",
        "pred = model(token_index)\n",
        "print(f'The sentiment score is {pred} - 0 is negative and 1 is positive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq_MBaMJYWNS"
      },
      "source": [
        "### Activity: Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03B5Zg9sY47y"
      },
      "source": [
        "#### Step 1: Import the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLhTrP3TYVTg"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_words = 10000  \n",
        "seq_length = 80  \n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=seq_length)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=seq_length)\n",
        "print('input_train shape:', x_train.shape)\n",
        "print('input_test shape:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oqR6rQkY72p"
      },
      "source": [
        "#### Step 2: Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h87iPusTYmdM"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow. keras.layers import LSTM\n",
        "\n",
        "hidden_size = 16\n",
        "embedding_size = 16\n",
        "L1 = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_size))\n",
        "model.add(LSTM(hidden_size,activation='tanh'))\n",
        "model.add(Dense(L1, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxyjQOCeY-V_"
      },
      "source": [
        "#### Step 3: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpoZ4AEgYrBf"
      },
      "source": [
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "batch = 250\n",
        "num_epoch = 10 \n",
        "history = model.fit(x_train, y_train, epochs=num_epoch, batch_size=batch,validation_data=(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAKRIBMOZI0X"
      },
      "source": [
        "#### Step 4: Evalaute the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9dwkqwTY05R"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epoch = range(len(loss))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch,loss,label='loss')\n",
        "plt.plot(epoch,val_loss,label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch,acc,label='acc')\n",
        "plt.plot(epoch,val_acc,label='val_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35TtEWanZLbG"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "\n",
        "#text = \"This movie is lousy and horrible I feel very sad\"\n",
        "#text =\" This movie is very good and nice I feel very happy\"\n",
        "# text = \"This movie is lousy and horrible It is wasting my money\"\n",
        "text =\"This movie is good worth the money I love it\"\n",
        "tokens = text.lower().split()\n",
        "token_index = [word_index[word] for word in tokens]\n",
        "token_index = np.array(token_index)\n",
        "token_index = token_index[np.newaxis,:]\n",
        "\n",
        "pred = model(token_index)\n",
        "print(f'The sentiment score is {pred} - 0 is negative and 1 is positive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsU9Sc9qZOL2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}